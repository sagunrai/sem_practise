{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 3: Text Summarization\n",
                "\n",
                "This notebook implements:\n",
                "- Using pretrained mBART model for multilingual summarization\n",
                "- Generating summaries in multiple lengths (small, medium, large)\n",
                "- Support for both Nepali and English text\n",
                "- Evaluation using ROUGE metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Transformers and PyTorch\n",
                "import torch\n",
                "from transformers import (\n",
                "    MBartForConditionalGeneration,\n",
                "    MBart50TokenizerFast,\n",
                "    pipeline\n",
                ")\n",
                "\n",
                "# ROUGE metrics\n",
                "from rouge_score import rouge_scorer\n",
                "\n",
                "# Check GPU\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "print(\"‚úì Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths\n",
                "BASE_DIR = Path(r'c:\\Users\\sagun\\Desktop\\news_project')\n",
                "DATA_DIR = BASE_DIR / 'data' / 'processed'\n",
                "MODEL_DIR = BASE_DIR / 'models' / 'summarizer'\n",
                "RESULTS_DIR = BASE_DIR / 'results' / 'summaries'\n",
                "\n",
                "# Create directories\n",
                "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
                "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Model configuration - Using pretrained mBART\n",
                "MODEL_NAME = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
                "\n",
                "# Summary length configurations (in words)\n",
                "SUMMARY_CONFIGS = {\n",
                "    'small': {'min_length': 30, 'max_length': 50},\n",
                "    'medium': {'min_length': 80, 'max_length': 150},\n",
                "    'large': {'min_length': 150, 'max_length': 300}\n",
                "}\n",
                "\n",
                "print(f\"Pretrained Model: {MODEL_NAME}\")\n",
                "print(f\"\\nSummary configurations:\")\n",
                "for size, config in SUMMARY_CONFIGS.items():\n",
                "    print(f\"  {size}: {config['min_length']}-{config['max_length']} tokens\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Pretrained Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load pretrained mBART model and tokenizer\n",
                "print(f\"Loading pretrained model: {MODEL_NAME}...\")\n",
                "print(\"This may take a few minutes...\\n\")\n",
                "\n",
                "tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_NAME)\n",
                "model = MBartForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
                "model.to(device)\n",
                "\n",
                "print(\"‚úì Model and tokenizer loaded successfully\")\n",
                "print(f\"Model parameters: {model.num_parameters():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test data for summarization\n",
                "with open(DATA_DIR / 'test_data.json', 'r', encoding='utf-8') as f:\n",
                "    test_data = json.load(f)\n",
                "\n",
                "test_df = pd.DataFrame(test_data)\n",
                "print(f\"‚úì Loaded {len(test_df)} articles for summarization\")\n",
                "\n",
                "# Select a subset for demonstration (to save time)\n",
                "sample_size = min(50, len(test_df))\n",
                "sample_df = test_df.sample(n=sample_size, random_state=42)\n",
                "print(f\"‚úì Using {sample_size} articles for demonstration\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Summarization Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_summary(text, size='medium', src_lang='ne_NP', tgt_lang='ne_NP'):\n",
                "    \"\"\"\n",
                "    Generate summary using pretrained mBART model\n",
                "    \n",
                "    Args:\n",
                "        text: Input text to summarize\n",
                "        size: 'small', 'medium', or 'large'\n",
                "        src_lang: Source language code (ne_NP for Nepali)\n",
                "        tgt_lang: Target language code\n",
                "    \"\"\"\n",
                "    # Set source language\n",
                "    tokenizer.src_lang = src_lang\n",
                "    \n",
                "    # Get configuration for summary size\n",
                "    config = SUMMARY_CONFIGS.get(size, SUMMARY_CONFIGS['medium'])\n",
                "    \n",
                "    # Tokenize input\n",
                "    inputs = tokenizer(\n",
                "        text,\n",
                "        return_tensors='pt',\n",
                "        max_length=1024,\n",
                "        truncation=True\n",
                "    ).to(device)\n",
                "    \n",
                "    # Generate summary\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        summary_ids = model.generate(\n",
                "            inputs['input_ids'],\n",
                "            num_beams=4,\n",
                "            min_length=config['min_length'],\n",
                "            max_length=config['max_length'],\n",
                "            early_stopping=True,\n",
                "            forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang]\n",
                "        )\n",
                "    \n",
                "    # Decode summary\n",
                "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
                "    \n",
                "    return summary\n",
                "\n",
                "# Test the function\n",
                "sample_text = sample_df.iloc[0]['text']\n",
                "print(\"Testing summarization function...\\n\")\n",
                "print(f\"Original text (first 300 chars):\\n{sample_text[:300]}...\\n\")\n",
                "\n",
                "for size in ['small', 'medium', 'large']:\n",
                "    summary = generate_summary(sample_text, size=size)\n",
                "    print(f\"{size.upper()} summary:\\n{summary}\\n\")\n",
                "    print(\"-\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Generate Summaries for All Sizes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate summaries for all articles in all sizes\n",
                "from tqdm import tqdm\n",
                "\n",
                "summaries_data = []\n",
                "\n",
                "print(\"Generating summaries for all articles...\\n\")\n",
                "\n",
                "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Summarizing\"):\n",
                "    text = row['text']\n",
                "    category = row['category']\n",
                "    \n",
                "    article_summaries = {\n",
                "        'original_text': text,\n",
                "        'category': category,\n",
                "        'text_length': len(text),\n",
                "        'word_count': len(text.split())\n",
                "    }\n",
                "    \n",
                "    # Generate summaries for each size\n",
                "    for size in ['small', 'medium', 'large']:\n",
                "        try:\n",
                "            summary = generate_summary(text, size=size)\n",
                "            article_summaries[f'{size}_summary'] = summary\n",
                "            article_summaries[f'{size}_length'] = len(summary)\n",
                "            article_summaries[f'{size}_words'] = len(summary.split())\n",
                "        except Exception as e:\n",
                "            print(f\"Error generating {size} summary for article {idx}: {e}\")\n",
                "            article_summaries[f'{size}_summary'] = \"\"\n",
                "    \n",
                "    summaries_data.append(article_summaries)\n",
                "\n",
                "print(\"\\n‚úì Summary generation complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create DataFrame with summaries\n",
                "summaries_df = pd.DataFrame(summaries_data)\n",
                "\n",
                "print(\"Summary Statistics:\\n\")\n",
                "print(f\"Total articles summarized: {len(summaries_df)}\")\n",
                "print(f\"\\nAverage summary lengths (characters):\")\n",
                "for size in ['small', 'medium', 'large']:\n",
                "    avg_length = summaries_df[f'{size}_length'].mean()\n",
                "    avg_words = summaries_df[f'{size}_words'].mean()\n",
                "    print(f\"  {size}: {avg_length:.0f} chars, {avg_words:.0f} words\")\n",
                "\n",
                "summaries_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualize Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare original text length vs summary lengths\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                "\n",
                "# Original text length distribution\n",
                "axes[0, 0].hist(summaries_df['text_length'], bins=30, color='steelblue', edgecolor='black')\n",
                "axes[0, 0].set_title('Original Text Length Distribution', fontweight='bold')\n",
                "axes[0, 0].set_xlabel('Length (characters)')\n",
                "axes[0, 0].set_ylabel('Frequency')\n",
                "\n",
                "# Summary lengths comparison\n",
                "summary_lengths = summaries_df[['small_length', 'medium_length', 'large_length']]\n",
                "summary_lengths.columns = ['Small', 'Medium', 'Large']\n",
                "summary_lengths.boxplot(ax=axes[0, 1])\n",
                "axes[0, 1].set_title('Summary Length Comparison', fontweight='bold')\n",
                "axes[0, 1].set_ylabel('Length (characters)')\n",
                "\n",
                "# Compression ratio\n",
                "for size in ['small', 'medium', 'large']:\n",
                "    summaries_df[f'{size}_compression'] = summaries_df[f'{size}_length'] / summaries_df['text_length'] * 100\n",
                "\n",
                "compression_data = summaries_df[['small_compression', 'medium_compression', 'large_compression']]\n",
                "compression_data.columns = ['Small', 'Medium', 'Large']\n",
                "compression_data.boxplot(ax=axes[1, 0])\n",
                "axes[1, 0].set_title('Compression Ratio (%)', fontweight='bold')\n",
                "axes[1, 0].set_ylabel('Percentage of Original')\n",
                "\n",
                "# Average summary length by size\n",
                "avg_lengths = [summaries_df[f'{size}_length'].mean() for size in ['small', 'medium', 'large']]\n",
                "axes[1, 1].bar(['Small', 'Medium', 'Large'], avg_lengths, color=['lightcoral', 'skyblue', 'lightgreen'])\n",
                "axes[1, 1].set_title('Average Summary Length by Size', fontweight='bold')\n",
                "axes[1, 1].set_ylabel('Length (characters)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(RESULTS_DIR / 'summary_statistics.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"‚úì Visualization saved to {RESULTS_DIR / 'summary_statistics.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. ROUGE Score Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For ROUGE evaluation, we'll use the medium summary as reference\n",
                "# and compare small and large summaries against it\n",
                "\n",
                "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
                "\n",
                "def calculate_rouge_scores(reference, hypothesis):\n",
                "    \"\"\"Calculate ROUGE scores\"\"\"\n",
                "    scores = scorer.score(reference, hypothesis)\n",
                "    return {\n",
                "        'rouge1': scores['rouge1'].fmeasure,\n",
                "        'rouge2': scores['rouge2'].fmeasure,\n",
                "        'rougeL': scores['rougeL'].fmeasure\n",
                "    }\n",
                "\n",
                "# Calculate ROUGE scores for a subset\n",
                "rouge_results = []\n",
                "\n",
                "for idx, row in summaries_df.head(20).iterrows():\n",
                "    # Use original text as reference (first 200 words)\n",
                "    reference = ' '.join(row['original_text'].split()[:200])\n",
                "    \n",
                "    for size in ['small', 'medium', 'large']:\n",
                "        hypothesis = row[f'{size}_summary']\n",
                "        if hypothesis:\n",
                "            scores = calculate_rouge_scores(reference, hypothesis)\n",
                "            scores['size'] = size\n",
                "            rouge_results.append(scores)\n",
                "\n",
                "rouge_df = pd.DataFrame(rouge_results)\n",
                "\n",
                "print(\"ROUGE Scores by Summary Size:\\n\")\n",
                "print(rouge_df.groupby('size')[['rouge1', 'rouge2', 'rougeL']].mean())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize ROUGE scores\n",
                "rouge_avg = rouge_df.groupby('size')[['rouge1', 'rouge2', 'rougeL']].mean()\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "rouge_avg.plot(kind='bar', ax=ax, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
                "ax.set_title('ROUGE Scores by Summary Size', fontsize=14, fontweight='bold')\n",
                "ax.set_xlabel('Summary Size')\n",
                "ax.set_ylabel('F1 Score')\n",
                "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
                "ax.legend(title='ROUGE Metric')\n",
                "ax.grid(axis='y', alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig(RESULTS_DIR / 'rouge_scores.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"‚úì ROUGE scores saved to {RESULTS_DIR / 'rouge_scores.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save all summaries\n",
                "summaries_output = summaries_df.to_dict('records')\n",
                "\n",
                "with open(RESULTS_DIR / 'all_summaries.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(summaries_output, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"‚úì All summaries saved to {RESULTS_DIR / 'all_summaries.json'}\")\n",
                "\n",
                "# Save ROUGE scores\n",
                "rouge_summary = rouge_df.groupby('size')[['rouge1', 'rouge2', 'rougeL']].mean().to_dict()\n",
                "\n",
                "with open(RESULTS_DIR / 'rouge_scores.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(rouge_summary, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"‚úì ROUGE scores saved to {RESULTS_DIR / 'rouge_scores.json'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Example Summaries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display example summaries\n",
                "print(\"=\"*80)\n",
                "print(\"EXAMPLE SUMMARIES\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "for i in range(min(3, len(summaries_df))):\n",
                "    row = summaries_df.iloc[i]\n",
                "    \n",
                "    print(f\"\\nüì∞ Article {i+1} - Category: {row['category']}\")\n",
                "    print(\"-\" * 80)\n",
                "    print(f\"\\nOriginal Text ({row['word_count']} words):\\n{row['original_text'][:300]}...\\n\")\n",
                "    \n",
                "    for size in ['small', 'medium', 'large']:\n",
                "        summary = row[f'{size}_summary']\n",
                "        words = row[f'{size}_words']\n",
                "        print(f\"\\n{size.upper()} Summary ({words} words):\\n{summary}\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"TEXT SUMMARIZATION SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "print(f\"\\nü§ñ Pretrained Model: {MODEL_NAME}\")\n",
                "print(f\"üìä Articles Summarized: {len(summaries_df)}\")\n",
                "print(f\"\\nüìù Summary Sizes:\")\n",
                "for size in ['small', 'medium', 'large']:\n",
                "    avg_words = summaries_df[f'{size}_words'].mean()\n",
                "    avg_compression = summaries_df[f'{size}_compression'].mean()\n",
                "    print(f\"  ‚Ä¢ {size.capitalize()}: ~{avg_words:.0f} words ({avg_compression:.1f}% of original)\")\n",
                "\n",
                "print(f\"\\nüìà ROUGE Scores (Medium Summary):\")\n",
                "medium_scores = rouge_df[rouge_df['size'] == 'medium'][['rouge1', 'rouge2', 'rougeL']].mean()\n",
                "print(f\"  ‚Ä¢ ROUGE-1: {medium_scores['rouge1']:.4f}\")\n",
                "print(f\"  ‚Ä¢ ROUGE-2: {medium_scores['rouge2']:.4f}\")\n",
                "print(f\"  ‚Ä¢ ROUGE-L: {medium_scores['rougeL']:.4f}\")\n",
                "\n",
                "print(f\"\\nüíæ Saved Files:\")\n",
                "print(f\"  ‚Ä¢ All summaries: {RESULTS_DIR / 'all_summaries.json'}\")\n",
                "print(f\"  ‚Ä¢ ROUGE scores: {RESULTS_DIR / 'rouge_scores.json'}\")\n",
                "print(f\"  ‚Ä¢ Statistics plot: {RESULTS_DIR / 'summary_statistics.png'}\")\n",
                "print(f\"  ‚Ä¢ ROUGE plot: {RESULTS_DIR / 'rouge_scores.png'}\")\n",
                "print(\"\\n‚úÖ Text summarization completed successfully!\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}