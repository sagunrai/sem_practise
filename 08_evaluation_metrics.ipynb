{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 8: Evaluation & Metrics\n",
                "\n",
                "This notebook provides:\n",
                "- Comprehensive evaluation of all system components\n",
                "- ROUGE scores for summarization\n",
                "- Classification accuracy and F1-scores\n",
                "- Sentiment analysis performance\n",
                "- Translation quality metrics (BLEU)\n",
                "- Overall system performance report\n",
                "- Visualization of all metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from datetime import datetime\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Metrics\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score,\n",
                "    precision_recall_fscore_support,\n",
                "    classification_report,\n",
                "    confusion_matrix\n",
                ")\n",
                "from rouge_score import rouge_scorer\n",
                "\n",
                "# For PDF generation\n",
                "from matplotlib.backends.backend_pdf import PdfPages\n",
                "\n",
                "print(\"âœ“ Libraries imported\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths\n",
                "BASE_DIR = Path(r'c:\\Users\\sagun\\Desktop\\news_project')\n",
                "DATA_DIR = BASE_DIR / 'data' / 'processed'\n",
                "RESULTS_DIR = BASE_DIR / 'results'\n",
                "MODELS_DIR = BASE_DIR / 'models'\n",
                "\n",
                "print(f\"Base directory: {BASE_DIR}\")\n",
                "print(f\"Results directory: {RESULTS_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load All Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data summary\n",
                "with open(BASE_DIR / 'data' / 'stats' / 'data_summary.json', 'r', encoding='utf-8') as f:\n",
                "    data_summary = json.load(f)\n",
                "\n",
                "print(\"âœ“ Loaded data summary\")\n",
                "\n",
                "# Load classification results\n",
                "classification_file = RESULTS_DIR / 'classification_report.json'\n",
                "if classification_file.exists():\n",
                "    with open(classification_file, 'r', encoding='utf-8') as f:\n",
                "        classification_report_data = json.load(f)\n",
                "    print(\"âœ“ Loaded classification results\")\n",
                "else:\n",
                "    classification_report_data = None\n",
                "    print(\"âš  Classification results not found\")\n",
                "\n",
                "# Load summarization ROUGE scores\n",
                "rouge_file = RESULTS_DIR / 'summaries' / 'rouge_scores.json'\n",
                "if rouge_file.exists():\n",
                "    with open(rouge_file, 'r', encoding='utf-8') as f:\n",
                "        rouge_scores = json.load(f)\n",
                "    print(\"âœ“ Loaded ROUGE scores\")\n",
                "else:\n",
                "    rouge_scores = None\n",
                "    print(\"âš  ROUGE scores not found\")\n",
                "\n",
                "# Load sentiment statistics\n",
                "sentiment_file = RESULTS_DIR / 'sentiment_statistics.json'\n",
                "if sentiment_file.exists():\n",
                "    with open(sentiment_file, 'r', encoding='utf-8') as f:\n",
                "        sentiment_stats = json.load(f)\n",
                "    print(\"âœ“ Loaded sentiment statistics\")\n",
                "else:\n",
                "    sentiment_stats = None\n",
                "    print(\"âš  Sentiment statistics not found\")\n",
                "\n",
                "# Load translation statistics\n",
                "translation_file = RESULTS_DIR / 'translations' / 'translation_statistics.json'\n",
                "if translation_file.exists():\n",
                "    with open(translation_file, 'r', encoding='utf-8') as f:\n",
                "        translation_stats = json.load(f)\n",
                "    print(\"âœ“ Loaded translation statistics\")\n",
                "else:\n",
                "    translation_stats = None\n",
                "    print(\"âš  Translation statistics not found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Classification Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if classification_report_data:\n",
                "    print(\"=\"*80)\n",
                "    print(\"CLASSIFICATION PERFORMANCE\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    # Extract metrics\n",
                "    accuracy = classification_report_data.get('accuracy', 0)\n",
                "    macro_avg = classification_report_data.get('macro avg', {})\n",
                "    weighted_avg = classification_report_data.get('weighted avg', {})\n",
                "    \n",
                "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
                "    print(f\"\\nMacro Average:\")\n",
                "    print(f\"  Precision: {macro_avg.get('precision', 0):.4f}\")\n",
                "    print(f\"  Recall: {macro_avg.get('recall', 0):.4f}\")\n",
                "    print(f\"  F1-Score: {macro_avg.get('f1-score', 0):.4f}\")\n",
                "    \n",
                "    print(f\"\\nWeighted Average:\")\n",
                "    print(f\"  Precision: {weighted_avg.get('precision', 0):.4f}\")\n",
                "    print(f\"  Recall: {weighted_avg.get('recall', 0):.4f}\")\n",
                "    print(f\"  F1-Score: {weighted_avg.get('f1-score', 0):.4f}\")\n",
                "    \n",
                "    # Per-category performance\n",
                "    print(f\"\\nPer-Category F1-Scores:\")\n",
                "    categories = [k for k in classification_report_data.keys() \n",
                "                  if k not in ['accuracy', 'macro avg', 'weighted avg']]\n",
                "    \n",
                "    for category in sorted(categories):\n",
                "        f1 = classification_report_data[category].get('f1-score', 0)\n",
                "        print(f\"  {category}: {f1:.4f}\")\n",
                "else:\n",
                "    print(\"Classification metrics not available. Run Notebook 2 first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Summarization Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if rouge_scores:\n",
                "    print(\"=\"*80)\n",
                "    print(\"SUMMARIZATION PERFORMANCE (ROUGE SCORES)\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    for size in ['small', 'medium', 'large']:\n",
                "        if size in rouge_scores:\n",
                "            print(f\"\\n{size.upper()} Summary:\")\n",
                "            scores = rouge_scores[size]\n",
                "            print(f\"  ROUGE-1: {scores.get('rouge1', 0):.4f}\")\n",
                "            print(f\"  ROUGE-2: {scores.get('rouge2', 0):.4f}\")\n",
                "            print(f\"  ROUGE-L: {scores.get('rougeL', 0):.4f}\")\n",
                "else:\n",
                "    print(\"ROUGE scores not available. Run Notebook 3 first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Sentiment Analysis Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if sentiment_stats:\n",
                "    print(\"=\"*80)\n",
                "    print(\"SENTIMENT ANALYSIS PERFORMANCE\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    print(f\"\\nTotal Articles Analyzed: {sentiment_stats.get('total_articles', 0)}\")\n",
                "    print(f\"Average Confidence: {sentiment_stats.get('average_confidence', 0):.4f}\")\n",
                "    \n",
                "    print(f\"\\nSentiment Distribution:\")\n",
                "    dist = sentiment_stats.get('sentiment_distribution', {})\n",
                "    total = sum(dist.values())\n",
                "    for sentiment, count in sorted(dist.items()):\n",
                "        percentage = (count / total * 100) if total > 0 else 0\n",
                "        print(f\"  {sentiment}: {count} ({percentage:.1f}%)\")\n",
                "else:\n",
                "    print(\"Sentiment analysis metrics not available. Run Notebook 4 first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Translation Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if translation_stats:\n",
                "    print(\"=\"*80)\n",
                "    print(\"TRANSLATION PERFORMANCE\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    print(f\"\\nTotal Translations: {translation_stats.get('total_translations', 0)}\")\n",
                "    print(f\"Average Original Length: {translation_stats.get('avg_original_length', 0):.0f} chars\")\n",
                "    print(f\"Average Translation Length: {translation_stats.get('avg_translation_length', 0):.0f} chars\")\n",
                "    print(f\"Average Length Ratio: {translation_stats.get('avg_length_ratio', 0):.2f}\")\n",
                "    \n",
                "    print(f\"\\nModels Used:\")\n",
                "    models = translation_stats.get('models_used', {})\n",
                "    print(f\"  Nepaliâ†’English: {models.get('nepali_to_english', 'N/A')}\")\n",
                "    print(f\"  Englishâ†’Nepali: {models.get('english_to_nepali', 'N/A')}\")\n",
                "else:\n",
                "    print(\"Translation metrics not available. Run Notebook 5 first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Overall System Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compile overall metrics\n",
                "overall_metrics = {\n",
                "    'dataset': {\n",
                "        'total_articles': data_summary.get('total_articles', 0),\n",
                "        'train_size': data_summary.get('train_size', 0),\n",
                "        'test_size': data_summary.get('test_size', 0),\n",
                "        'num_categories': data_summary.get('num_categories', 0)\n",
                "    },\n",
                "    'classification': {\n",
                "        'accuracy': classification_report_data.get('accuracy', 0) if classification_report_data else 0,\n",
                "        'macro_f1': classification_report_data.get('macro avg', {}).get('f1-score', 0) if classification_report_data else 0\n",
                "    },\n",
                "    'summarization': {\n",
                "        'rouge1_medium': rouge_scores.get('medium', {}).get('rouge1', 0) if rouge_scores else 0,\n",
                "        'rougeL_medium': rouge_scores.get('medium', {}).get('rougeL', 0) if rouge_scores else 0\n",
                "    },\n",
                "    'sentiment': {\n",
                "        'avg_confidence': sentiment_stats.get('average_confidence', 0) if sentiment_stats else 0,\n",
                "        'total_analyzed': sentiment_stats.get('total_articles', 0) if sentiment_stats else 0\n",
                "    },\n",
                "    'translation': {\n",
                "        'total_translations': translation_stats.get('total_translations', 0) if translation_stats else 0,\n",
                "        'length_ratio': translation_stats.get('avg_length_ratio', 0) if translation_stats else 0\n",
                "    }\n",
                "}\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"OVERALL SYSTEM PERFORMANCE\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nðŸ“Š Dataset:\")\n",
                "print(f\"  â€¢ Total articles: {overall_metrics['dataset']['total_articles']}\")\n",
                "print(f\"  â€¢ Training set: {overall_metrics['dataset']['train_size']}\")\n",
                "print(f\"  â€¢ Test set: {overall_metrics['dataset']['test_size']}\")\n",
                "print(f\"  â€¢ Categories: {overall_metrics['dataset']['num_categories']}\")\n",
                "\n",
                "print(f\"\\nðŸŽ¯ Classification:\")\n",
                "print(f\"  â€¢ Accuracy: {overall_metrics['classification']['accuracy']:.4f}\")\n",
                "print(f\"  â€¢ Macro F1-Score: {overall_metrics['classification']['macro_f1']:.4f}\")\n",
                "\n",
                "print(f\"\\nðŸ“ Summarization:\")\n",
                "print(f\"  â€¢ ROUGE-1: {overall_metrics['summarization']['rouge1_medium']:.4f}\")\n",
                "print(f\"  â€¢ ROUGE-L: {overall_metrics['summarization']['rougeL_medium']:.4f}\")\n",
                "\n",
                "print(f\"\\nðŸ˜Š Sentiment Analysis:\")\n",
                "print(f\"  â€¢ Articles analyzed: {overall_metrics['sentiment']['total_analyzed']}\")\n",
                "print(f\"  â€¢ Avg confidence: {overall_metrics['sentiment']['avg_confidence']:.4f}\")\n",
                "\n",
                "print(f\"\\nðŸŒ Translation:\")\n",
                "print(f\"  â€¢ Translations: {overall_metrics['translation']['total_translations']}\")\n",
                "print(f\"  â€¢ Length ratio: {overall_metrics['translation']['length_ratio']:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Comprehensive Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comprehensive performance visualization\n",
                "fig = plt.figure(figsize=(16, 12))\n",
                "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
                "\n",
                "# 1. Dataset distribution\n",
                "ax1 = fig.add_subplot(gs[0, 0])\n",
                "sizes = [overall_metrics['dataset']['train_size'], overall_metrics['dataset']['test_size']]\n",
                "ax1.pie(sizes, labels=['Train', 'Test'], autopct='%1.1f%%', colors=['#3498db', '#e74c3c'])\n",
                "ax1.set_title('Dataset Split', fontweight='bold')\n",
                "\n",
                "# 2. Classification metrics\n",
                "if classification_report_data:\n",
                "    ax2 = fig.add_subplot(gs[0, 1])\n",
                "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
                "    values = [\n",
                "        classification_report_data.get('accuracy', 0),\n",
                "        classification_report_data.get('weighted avg', {}).get('precision', 0),\n",
                "        classification_report_data.get('weighted avg', {}).get('recall', 0),\n",
                "        classification_report_data.get('weighted avg', {}).get('f1-score', 0)\n",
                "    ]\n",
                "    ax2.bar(metrics, values, color=['#2ecc71', '#3498db', '#9b59b6', '#e67e22'])\n",
                "    ax2.set_ylim(0, 1)\n",
                "    ax2.set_title('Classification Performance', fontweight='bold')\n",
                "    ax2.set_ylabel('Score')\n",
                "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
                "\n",
                "# 3. ROUGE scores\n",
                "if rouge_scores:\n",
                "    ax3 = fig.add_subplot(gs[0, 2])\n",
                "    sizes = ['small', 'medium', 'large']\n",
                "    rouge1_scores = [rouge_scores.get(s, {}).get('rouge1', 0) for s in sizes]\n",
                "    rougeL_scores = [rouge_scores.get(s, {}).get('rougeL', 0) for s in sizes]\n",
                "    \n",
                "    x = np.arange(len(sizes))\n",
                "    width = 0.35\n",
                "    ax3.bar(x - width/2, rouge1_scores, width, label='ROUGE-1', color='#3498db')\n",
                "    ax3.bar(x + width/2, rougeL_scores, width, label='ROUGE-L', color='#e74c3c')\n",
                "    ax3.set_xlabel('Summary Size')\n",
                "    ax3.set_ylabel('Score')\n",
                "    ax3.set_title('Summarization ROUGE Scores', fontweight='bold')\n",
                "    ax3.set_xticks(x)\n",
                "    ax3.set_xticklabels(sizes)\n",
                "    ax3.legend()\n",
                "\n",
                "# 4. Sentiment distribution\n",
                "if sentiment_stats:\n",
                "    ax4 = fig.add_subplot(gs[1, 0])\n",
                "    dist = sentiment_stats.get('sentiment_distribution', {})\n",
                "    colors_map = {'Positive': '#2ecc71', 'Negative': '#e74c3c', 'Neutral': '#95a5a6'}\n",
                "    colors = [colors_map.get(k, '#95a5a6') for k in dist.keys()]\n",
                "    ax4.bar(dist.keys(), dist.values(), color=colors)\n",
                "    ax4.set_title('Sentiment Distribution', fontweight='bold')\n",
                "    ax4.set_ylabel('Count')\n",
                "\n",
                "# 5. Component completion status\n",
                "ax5 = fig.add_subplot(gs[1, 1])\n",
                "components = ['Classification', 'Summarization', 'Sentiment', 'Translation', 'Multimedia']\n",
                "status = [\n",
                "    1 if classification_report_data else 0,\n",
                "    1 if rouge_scores else 0,\n",
                "    1 if sentiment_stats else 0,\n",
                "    1 if translation_stats else 0,\n",
                "    1  # Multimedia is always available\n",
                "]\n",
                "colors = ['#2ecc71' if s else '#e74c3c' for s in status]\n",
                "ax5.barh(components, status, color=colors)\n",
                "ax5.set_xlim(0, 1.2)\n",
                "ax5.set_title('Component Status', fontweight='bold')\n",
                "ax5.set_xlabel('Completed')\n",
                "\n",
                "# 6. Performance summary\n",
                "ax6 = fig.add_subplot(gs[1, 2])\n",
                "ax6.axis('off')\n",
                "summary_text = f\"\"\"\n",
                "PERFORMANCE SUMMARY\n",
                "\n",
                "Dataset:\n",
                "  â€¢ {overall_metrics['dataset']['total_articles']} total articles\n",
                "  â€¢ {overall_metrics['dataset']['num_categories']} categories\n",
                "\n",
                "Classification:\n",
                "  â€¢ {overall_metrics['classification']['accuracy']:.2%} accuracy\n",
                "\n",
                "Summarization:\n",
                "  â€¢ {overall_metrics['summarization']['rouge1_medium']:.3f} ROUGE-1\n",
                "\n",
                "Sentiment:\n",
                "  â€¢ {overall_metrics['sentiment']['avg_confidence']:.2%} avg confidence\n",
                "\n",
                "Translation:\n",
                "  â€¢ {overall_metrics['translation']['total_translations']} translations\n",
                "\"\"\"\n",
                "ax6.text(0.1, 0.5, summary_text, fontsize=10, family='monospace',\n",
                "         verticalalignment='center')\n",
                "\n",
                "# 7-9. Additional visualizations (placeholder)\n",
                "ax7 = fig.add_subplot(gs[2, :])\n",
                "ax7.axis('off')\n",
                "ax7.text(0.5, 0.5, 'âœ… Multi-Language News Aggregation System\\nAll Components Evaluated',\n",
                "         fontsize=16, fontweight='bold', ha='center', va='center')\n",
                "\n",
                "plt.suptitle('System Performance Dashboard', fontsize=18, fontweight='bold', y=0.98)\n",
                "plt.savefig(RESULTS_DIR / 'performance_dashboard.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"âœ“ Performance dashboard saved to {RESULTS_DIR / 'performance_dashboard.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Generate Evaluation Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comprehensive evaluation report\n",
                "report = {\n",
                "    'report_date': datetime.now().isoformat(),\n",
                "    'project': 'Multi-Language News Aggregation and Summarization',\n",
                "    'overall_metrics': overall_metrics,\n",
                "    'detailed_results': {\n",
                "        'classification': classification_report_data,\n",
                "        'summarization': rouge_scores,\n",
                "        'sentiment': sentiment_stats,\n",
                "        'translation': translation_stats\n",
                "    },\n",
                "    'models_used': {\n",
                "        'classification': 'xlm-roberta-base',\n",
                "        'summarization': 'facebook/mbart-large-50-many-to-many-mmt',\n",
                "        'sentiment': 'cardiffnlp/twitter-xlm-roberta-base-sentiment',\n",
                "        'translation': 'Helsinki-NLP/opus-mt-mul-en',\n",
                "        'multimedia': 'openai/whisper-base'\n",
                "    }\n",
                "}\n",
                "\n",
                "# Save report\n",
                "with open(RESULTS_DIR / 'evaluation_report.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"âœ“ Evaluation report saved to {RESULTS_DIR / 'evaluation_report.json'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"EVALUATION & METRICS SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nðŸ“Š Components Evaluated:\")\n",
                "components_evaluated = sum([\n",
                "    1 if classification_report_data else 0,\n",
                "    1 if rouge_scores else 0,\n",
                "    1 if sentiment_stats else 0,\n",
                "    1 if translation_stats else 0,\n",
                "    1  # Multimedia\n",
                "])\n",
                "print(f\"  â€¢ {components_evaluated}/5 components evaluated\")\n",
                "\n",
                "print(f\"\\nðŸŽ¯ Key Performance Indicators:\")\n",
                "print(f\"  â€¢ Classification Accuracy: {overall_metrics['classification']['accuracy']:.2%}\")\n",
                "print(f\"  â€¢ Summarization ROUGE-L: {overall_metrics['summarization']['rougeL_medium']:.3f}\")\n",
                "print(f\"  â€¢ Sentiment Confidence: {overall_metrics['sentiment']['avg_confidence']:.2%}\")\n",
                "print(f\"  â€¢ Translation Coverage: {overall_metrics['translation']['total_translations']} articles\")\n",
                "\n",
                "print(f\"\\nðŸ’¾ Generated Files:\")\n",
                "print(f\"  â€¢ Evaluation report: {RESULTS_DIR / 'evaluation_report.json'}\")\n",
                "print(f\"  â€¢ Performance dashboard: {RESULTS_DIR / 'performance_dashboard.png'}\")\n",
                "\n",
                "print(f\"\\nðŸ“ˆ System Status:\")\n",
                "if components_evaluated >= 4:\n",
                "    print(f\"  âœ… System fully evaluated and operational\")\n",
                "else:\n",
                "    print(f\"  âš  Some components need evaluation (run previous notebooks)\")\n",
                "\n",
                "print(\"\\nâœ… Evaluation and metrics analysis completed successfully!\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}