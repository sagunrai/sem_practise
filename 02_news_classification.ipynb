{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 2: News Classification\n",
                "\n",
                "This notebook implements:\n",
                "- Loading pretrained multilingual models (XLM-RoBERTa)\n",
                "- Fine-tuning for Nepali news classification\n",
                "- Training on 13+ news categories\n",
                "- Model evaluation and saving\n",
                "- Prediction on new articles"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting numpy\n",
                        "  Downloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
                        "Collecting pandas\n",
                        "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
                        "Collecting matplotlib\n",
                        "  Downloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
                        "Collecting seaborn\n",
                        "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
                        "Collecting scikit-learn\n",
                        "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
                        "Collecting torch\n",
                        "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
                        "Collecting transformers\n",
                        "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
                        "Collecting pytz>=2020.1 (from pandas)\n",
                        "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
                        "Collecting contourpy>=1.0.1 (from matplotlib)\n",
                        "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
                        "Collecting cycler>=0.10 (from matplotlib)\n",
                        "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
                        "Collecting fonttools>=4.22.0 (from matplotlib)\n",
                        "  Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (114 kB)\n",
                        "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
                        "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
                        "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
                        "Collecting pillow>=8 (from matplotlib)\n",
                        "  Downloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
                        "Collecting pyparsing>=3 (from matplotlib)\n",
                        "  Downloading pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
                        "Collecting scipy>=1.10.0 (from scikit-learn)\n",
                        "  Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
                        "Collecting joblib>=1.3.0 (from scikit-learn)\n",
                        "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
                        "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
                        "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
                        "Collecting filelock (from torch)\n",
                        "  Downloading filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
                        "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.15.0)\n",
                        "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (80.9.0)\n",
                        "Collecting sympy>=1.13.3 (from torch)\n",
                        "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
                        "Collecting networkx>=2.5.1 (from torch)\n",
                        "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
                        "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
                        "Collecting fsspec>=0.8.5 (from torch)\n",
                        "  Downloading fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\n",
                        "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
                        "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
                        "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
                        "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
                        "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
                        "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
                        "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
                        "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
                        "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
                        "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
                        "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
                        "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
                        "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
                        "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
                        "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
                        "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
                        "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
                        "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
                        "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
                        "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
                        "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
                        "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
                        "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
                        "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
                        "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
                        "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
                        "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
                        "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
                        "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
                        "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
                        "Collecting triton==3.5.1 (from torch)\n",
                        "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
                        "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
                        "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
                        "Collecting regex!=2019.12.17 (from transformers)\n",
                        "  Downloading regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
                        "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
                        "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
                        "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
                        "Collecting safetensors>=0.4.3 (from transformers)\n",
                        "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
                        "Collecting tqdm>=4.27 (from transformers)\n",
                        "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
                        "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
                        "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
                        "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
                        "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
                        "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2025.11.12)\n",
                        "Downloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
                        "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
                        "Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m  \u001b[33m0:00:25\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m  \u001b[33m0:00:16\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m  \u001b[33m0:00:20\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
                        "Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
                        "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
                        "Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (5.0 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
                        "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
                        "Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
                        "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
                        "Downloading regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
                        "Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
                        "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
                        "Downloading filelock-3.20.3-py3-none-any.whl (16 kB)\n",
                        "Installing collected packages: pytz, nvidia-cusparselt-cu12, mpmath, triton, tqdm, threadpoolctl, sympy, safetensors, regex, pyparsing, pillow, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, kiwisolver, joblib, hf-xet, fsspec, fonttools, filelock, cycler, scipy, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, contourpy, tokenizers, scikit-learn, nvidia-cusolver-cu12, matplotlib, transformers, torch, seaborn\n",
                        "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/44\u001b[0m [triton]\u001b[33m  WARNING: The scripts proton and proton-viewer are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
                        "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
                        "\u001b[0m\u001b[33m  WARNING: The script tqdm is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
                        "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
                        "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/44\u001b[0m [sympy]\u001b[33m  WARNING: The script isympy is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
                        "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
                        "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/44\u001b[0m [numpy]-cublas-cu12]u12]2]\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
                        "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
                        "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/44\u001b[0m [fonttools]]\u001b[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
                        "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
                        "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m35/44\u001b[0m [huggingface-hub]2]12]\u001b[33m  WARNING: The scripts hf, huggingface-cli and tiny-agents are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
                        "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
                        "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m41/44\u001b[0m [transformers]er-cu12]\u001b[33m  WARNING: The scripts transformers and transformers-cli are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
                        "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
                        "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m42/44\u001b[0m [torch]\u001b[33m  WARNING: The scripts torchfrtrace and torchrun are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
                        "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44/44\u001b[0m [seaborn][seaborn]\n",
                        "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 filelock-3.20.3 fonttools-4.61.1 fsspec-2026.1.0 hf-xet-1.2.0 huggingface-hub-0.36.0 joblib-1.5.3 kiwisolver-1.4.9 matplotlib-3.10.8 mpmath-1.3.0 networkx-3.6.1 numpy-2.4.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 pandas-2.3.3 pillow-12.1.0 pyparsing-3.3.1 pytz-2025.2 regex-2026.1.15 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.17.0 seaborn-0.13.2 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.2 torch-2.9.1 tqdm-4.67.1 transformers-4.57.6 triton-3.5.1\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "%pip install numpy pandas matplotlib seaborn scikit-learn torch transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting datasets\n",
                        "  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
                        "Collecting evaluate\n",
                        "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
                        "Requirement already satisfied: filelock in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (3.20.3)\n",
                        "Requirement already satisfied: numpy>=1.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (2.4.1)\n",
                        "Collecting pyarrow>=21.0.0 (from datasets)\n",
                        "  Downloading pyarrow-23.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
                        "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
                        "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
                        "Requirement already satisfied: pandas in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
                        "Requirement already satisfied: requests>=2.32.2 in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
                        "Requirement already satisfied: httpx<1.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
                        "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
                        "Collecting xxhash (from datasets)\n",
                        "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
                        "Collecting multiprocess<0.70.19 (from datasets)\n",
                        "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
                        "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
                        "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
                        "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (0.36.0)\n",
                        "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
                        "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
                        "  Downloading aiohttp-3.13.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
                        "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
                        "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
                        "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
                        "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
                        "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
                        "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
                        "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
                        "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
                        "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
                        "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
                        "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
                        "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
                        "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
                        "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
                        "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
                        "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
                        "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
                        "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
                        "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
                        "Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
                        "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
                        "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
                        "Downloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
                        "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
                        "Downloading aiohttp-3.13.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
                        "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
                        "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
                        "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
                        "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
                        "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
                        "Downloading pyarrow-23.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.6 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
                        "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets, evaluate\n",
                        "\u001b[2K  Attempting uninstall: fsspec━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/14\u001b[0m [pyarrow]\n",
                        "\u001b[2K    Found existing installation: fsspec 2026.1.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/14\u001b[0m [pyarrow]\n",
                        "\u001b[2K    Uninstalling fsspec-2026.1.0:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/14\u001b[0m [fsspec]\n",
                        "\u001b[2K      Successfully uninstalled fsspec-2026.1.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/14\u001b[0m [fsspec]\n",
                        "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m12/14\u001b[0m [datasets]ess]\u001b[33m  WARNING: The script datasets-cli is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
                        "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
                        "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m13/14\u001b[0m [evaluate]\u001b[33m  WARNING: The script evaluate-cli is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
                        "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [evaluate]\n",
                        "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 datasets-4.5.0 dill-0.4.0 evaluate-0.4.6 frozenlist-1.8.0 fsspec-2025.10.0 multidict-6.7.0 multiprocess-0.70.18 propcache-0.4.1 pyarrow-23.0.0 xxhash-3.6.0 yarl-1.22.0\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "%pip install datasets evaluate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n",
                        "\n",
                        "✓ Libraries imported successfully\n"
                    ]
                }
            ],
            "source": [
                "# Import required libraries\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Transformers and PyTorch\n",
                "import torch\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForSequenceClassification,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorWithPadding\n",
                ")\n",
                "from datasets import Dataset\n",
                "\n",
                "# Check GPU availability\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "print(\"\\n✓ Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: FacebookAI/xlm-roberta-base\n",
                        "Max sequence length: 512\n",
                        "Batch size: 8\n",
                        "Learning rate: 2e-05\n",
                        "Number of epochs: 3\n"
                    ]
                }
            ],
            "source": [
                "# Paths\n",
                "BASE_DIR = Path(r'/workspaces/sem_practise')\n",
                "DATA_DIR = BASE_DIR / 'data' \n",
                "MODEL_DIR = BASE_DIR / 'models' / 'news_classifier'\n",
                "RESULTS_DIR = BASE_DIR / 'results'\n",
                "\n",
                "# Create directories\n",
                "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
                "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Model configuration\n",
                "MODEL_NAME = \"FacebookAI/xlm-roberta-base\"  # Pretrained multilingual model\n",
                "MAX_LENGTH = 512\n",
                "BATCH_SIZE = 8\n",
                "LEARNING_RATE = 2e-5\n",
                "NUM_EPOCHS = 3\n",
                "\n",
                "print(f\"Model: {MODEL_NAME}\")\n",
                "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
                "print(f\"Batch size: {BATCH_SIZE}\")\n",
                "print(f\"Learning rate: {LEARNING_RATE}\")\n",
                "print(f\"Number of epochs: {NUM_EPOCHS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Loaded 1431 training samples\n",
                        "✓ Loaded 358 test samples\n",
                        "\n",
                        "Sample data:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>text</th>\n",
                            "      <th>category</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>३० पुस, काठमाडौं । दोर्दी खोला जलविद्मुत कम्पन...</td>\n",
                            "      <td>bank</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>के तपाईंलाई थाहा छ ? हाथ, खुट्टाका नङहरुको रंग...</td>\n",
                            "      <td>health</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>प्रधानमन्त्री केपी शर्मा ओली थ्री नेसन्स कपको ...</td>\n",
                            "      <td>sports</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>राजविराज, २४ चैत । सप्तरीको मलेकपुर गाउँ विकास...</td>\n",
                            "      <td>politic</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>सेञ्चुरीमा गत असोज १ गतेदेखि प्रमूख कार्यकारी ...</td>\n",
                            "      <td>bank</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                text category\n",
                            "0  ३० पुस, काठमाडौं । दोर्दी खोला जलविद्मुत कम्पन...     bank\n",
                            "1  के तपाईंलाई थाहा छ ? हाथ, खुट्टाका नङहरुको रंग...   health\n",
                            "2  प्रधानमन्त्री केपी शर्मा ओली थ्री नेसन्स कपको ...   sports\n",
                            "3  राजविराज, २४ चैत । सप्तरीको मलेकपुर गाउँ विकास...  politic\n",
                            "4  सेञ्चुरीमा गत असोज १ गतेदेखि प्रमूख कार्यकारी ...     bank"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Load preprocessed data\n",
                "with open(DATA_DIR / 'train_data.json', 'r', encoding='utf-8') as f:\n",
                "    train_data = json.load(f)\n",
                "\n",
                "with open(DATA_DIR / 'test_data.json', 'r', encoding='utf-8') as f:\n",
                "    test_data = json.load(f)\n",
                "\n",
                "print(f\"✓ Loaded {len(train_data)} training samples\")\n",
                "print(f\"✓ Loaded {len(test_data)} test samples\")\n",
                "\n",
                "# Convert to DataFrames for easier handling\n",
                "train_df = pd.DataFrame(train_data)\n",
                "test_df = pd.DataFrame(test_data)\n",
                "\n",
                "print(\"\\nSample data:\")\n",
                "train_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of categories: 13\n",
                        "\n",
                        "Category mappings:\n",
                        "  0: Agriculture\n",
                        "  1: automobiles\n",
                        "  2: bank\n",
                        "  3: business\n",
                        "  4: economy\n",
                        "  5: education\n",
                        "  6: entertainment\n",
                        "  7: health\n",
                        "  8: politic\n",
                        "  9: sports\n",
                        "  10: technology\n",
                        "  11: tourism\n",
                        "  12: world\n",
                        "\n",
                        "✓ Label mappings saved to /workspaces/sem_practise/models/news_classifier/label_mappings.json\n"
                    ]
                }
            ],
            "source": [
                "# Create label mappings\n",
                "categories = sorted(train_df['category'].unique())\n",
                "label2id = {label: idx for idx, label in enumerate(categories)}\n",
                "id2label = {idx: label for label, idx in label2id.items()}\n",
                "\n",
                "print(f\"Number of categories: {len(categories)}\")\n",
                "print(\"\\nCategory mappings:\")\n",
                "for label, idx in label2id.items():\n",
                "    print(f\"  {idx}: {label}\")\n",
                "\n",
                "# Add numeric labels\n",
                "train_df['label'] = train_df['category'].map(label2id)\n",
                "test_df['label'] = test_df['category'].map(label2id)\n",
                "\n",
                "# Save label mappings\n",
                "with open(MODEL_DIR / 'label_mappings.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump({'label2id': label2id, 'id2label': id2label}, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"\\n✓ Label mappings saved to {MODEL_DIR / 'label_mappings.json'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Pretrained Model and Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading tokenizer: FacebookAI/xlm-roberta-base...\n",
                        "✓ Tokenizer loaded\n",
                        "\n",
                        "Loading pretrained model: FacebookAI/xlm-roberta-base...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Model loaded and moved to device\n",
                        "\n",
                        "Model parameters: 278,053,645\n"
                    ]
                }
            ],
            "source": [
                "# Load tokenizer\n",
                "print(f\"Loading tokenizer: {MODEL_NAME}...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "print(\"✓ Tokenizer loaded\")\n",
                "\n",
                "# Load pretrained model for sequence classification\n",
                "print(f\"\\nLoading pretrained model: {MODEL_NAME}...\")\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    num_labels=len(categories),\n",
                "    id2label=id2label,\n",
                "    label2id=label2id\n",
                ")\n",
                "model.to(device)\n",
                "print(\"✓ Model loaded and moved to device\")\n",
                "\n",
                "# Display model info\n",
                "print(f\"\\nModel parameters: {model.num_parameters():,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Tokenize Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tokenize_function(examples):\n",
                "    \"\"\"Tokenize text data\"\"\"\n",
                "    return tokenizer(\n",
                "        examples['text'],\n",
                "        padding='max_length',\n",
                "        truncation=True,\n",
                "        max_length=MAX_LENGTH\n",
                "    )\n",
                "\n",
                "# Convert to HuggingFace Dataset format\n",
                "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
                "test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
                "\n",
                "print(\"Tokenizing training data...\")\n",
                "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
                "\n",
                "print(\"Tokenizing test data...\")\n",
                "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
                "\n",
                "print(\"\\n✓ Tokenization complete\")\n",
                "print(f\"Train dataset: {train_dataset}\")\n",
                "print(f\"Test dataset: {test_dataset}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=str(MODEL_DIR / 'checkpoints'),\n",
                "    num_train_epochs=NUM_EPOCHS,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    per_device_eval_batch_size=BATCH_SIZE,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    weight_decay=0.01,\n",
                "    evaluation_strategy='epoch',\n",
                "    save_strategy='epoch',\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model='accuracy',\n",
                "    logging_dir=str(MODEL_DIR / 'logs'),\n",
                "    logging_steps=10,\n",
                "    warmup_steps=100,\n",
                "    save_total_limit=2,\n",
                "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
                ")\n",
                "\n",
                "print(\"Training configuration:\")\n",
                "print(f\"  Output directory: {training_args.output_dir}\")\n",
                "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
                "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
                "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
                "print(f\"  Mixed precision (FP16): {training_args.fp16}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define metrics\n",
                "def compute_metrics(eval_pred):\n",
                "    \"\"\"Compute accuracy and other metrics\"\"\"\n",
                "    predictions, labels = eval_pred\n",
                "    predictions = np.argmax(predictions, axis=1)\n",
                "    accuracy = accuracy_score(labels, predictions)\n",
                "    return {'accuracy': accuracy}\n",
                "\n",
                "# Data collator\n",
                "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
                "\n",
                "print(\"✓ Metrics and data collator configured\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Fine-tune Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=test_dataset,\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics,\n",
                ")\n",
                "\n",
                "print(\"✓ Trainer initialized\")\n",
                "print(\"\\nStarting fine-tuning...\\n\")\n",
                "\n",
                "# Train the model\n",
                "train_result = trainer.train()\n",
                "\n",
                "print(\"\\n✓ Training complete!\")\n",
                "print(f\"Training loss: {train_result.training_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Evaluate Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set\n",
                "print(\"Evaluating model on test set...\")\n",
                "eval_results = trainer.evaluate()\n",
                "\n",
                "print(\"\\nEvaluation Results:\")\n",
                "for key, value in eval_results.items():\n",
                "    print(f\"  {key}: {value:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get predictions\n",
                "predictions = trainer.predict(test_dataset)\n",
                "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
                "true_labels = test_df['label'].values\n",
                "\n",
                "# Classification report\n",
                "print(\"\\nDetailed Classification Report:\")\n",
                "print(\"=\"*80)\n",
                "report = classification_report(\n",
                "    true_labels,\n",
                "    pred_labels,\n",
                "    target_names=categories,\n",
                "    digits=4\n",
                ")\n",
                "print(report)\n",
                "\n",
                "# Save classification report\n",
                "report_dict = classification_report(\n",
                "    true_labels,\n",
                "    pred_labels,\n",
                "    target_names=categories,\n",
                "    output_dict=True\n",
                ")\n",
                "\n",
                "with open(RESULTS_DIR / 'classification_report.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(report_dict, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"\\n✓ Report saved to {RESULTS_DIR / 'classification_report.json'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "cm = confusion_matrix(true_labels, pred_labels)\n",
                "\n",
                "plt.figure(figsize=(14, 12))\n",
                "sns.heatmap(\n",
                "    cm,\n",
                "    annot=True,\n",
                "    fmt='d',\n",
                "    cmap='Blues',\n",
                "    xticklabels=categories,\n",
                "    yticklabels=categories,\n",
                "    cbar_kws={'label': 'Count'}\n",
                ")\n",
                "plt.title('Confusion Matrix - News Classification', fontsize=16, fontweight='bold', pad=20)\n",
                "plt.xlabel('Predicted Category', fontsize=12)\n",
                "plt.ylabel('True Category', fontsize=12)\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.yticks(rotation=0)\n",
                "plt.tight_layout()\n",
                "plt.savefig(RESULTS_DIR / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"✓ Confusion matrix saved to {RESULTS_DIR / 'confusion_matrix.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Fine-tuned Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the fine-tuned model\n",
                "print(\"Saving fine-tuned model...\")\n",
                "trainer.save_model(str(MODEL_DIR / 'final_model'))\n",
                "tokenizer.save_pretrained(str(MODEL_DIR / 'final_model'))\n",
                "\n",
                "print(f\"✓ Model saved to {MODEL_DIR / 'final_model'}\")\n",
                "\n",
                "# Save training metrics\n",
                "metrics = {\n",
                "    'model_name': MODEL_NAME,\n",
                "    'num_categories': len(categories),\n",
                "    'train_samples': len(train_df),\n",
                "    'test_samples': len(test_df),\n",
                "    'accuracy': float(eval_results['eval_accuracy']),\n",
                "    'training_loss': float(train_result.training_loss),\n",
                "    'epochs': NUM_EPOCHS,\n",
                "    'batch_size': BATCH_SIZE,\n",
                "    'learning_rate': LEARNING_RATE\n",
                "}\n",
                "\n",
                "with open(MODEL_DIR / 'training_metrics.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"✓ Metrics saved to {MODEL_DIR / 'training_metrics.json'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Test Predictions on Sample Articles"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_category(text, model, tokenizer, id2label, device):\n",
                "    \"\"\"\n",
                "    Predict category for a given text\n",
                "    \"\"\"\n",
                "    # Tokenize\n",
                "    inputs = tokenizer(\n",
                "        text,\n",
                "        return_tensors='pt',\n",
                "        truncation=True,\n",
                "        max_length=MAX_LENGTH,\n",
                "        padding=True\n",
                "    ).to(device)\n",
                "    \n",
                "    # Predict\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
                "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
                "        confidence = predictions[0][predicted_class].item()\n",
                "    \n",
                "    return id2label[str(predicted_class)], confidence\n",
                "\n",
                "# Test on random samples\n",
                "print(\"Testing predictions on random samples:\\n\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "sample_indices = np.random.choice(len(test_df), 5, replace=False)\n",
                "\n",
                "for idx in sample_indices:\n",
                "    text = test_df.iloc[idx]['text']\n",
                "    true_category = test_df.iloc[idx]['category']\n",
                "    \n",
                "    predicted_category, confidence = predict_category(text, model, tokenizer, id2label, device)\n",
                "    \n",
                "    print(f\"\\nText (first 150 chars): {text[:150]}...\")\n",
                "    print(f\"True Category: {true_category}\")\n",
                "    print(f\"Predicted Category: {predicted_category}\")\n",
                "    print(f\"Confidence: {confidence:.4f}\")\n",
                "    print(f\"Correct: {'✓' if true_category == predicted_category else '✗'}\")\n",
                "    print(\"-\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"NEWS CLASSIFICATION SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "print(f\"\\n🤖 Model: {MODEL_NAME} (Pretrained)\")\n",
                "print(f\"📊 Categories: {len(categories)}\")\n",
                "print(f\"📝 Training samples: {len(train_df)}\")\n",
                "print(f\"🧪 Test samples: {len(test_df)}\")\n",
                "print(f\"\\n📈 Performance:\")\n",
                "print(f\"  • Accuracy: {metrics['accuracy']:.4f}\")\n",
                "print(f\"  • Training loss: {metrics['training_loss']:.4f}\")\n",
                "print(f\"\\n💾 Saved Files:\")\n",
                "print(f\"  • Model: {MODEL_DIR / 'final_model'}\")\n",
                "print(f\"  • Label mappings: {MODEL_DIR / 'label_mappings.json'}\")\n",
                "print(f\"  • Training metrics: {MODEL_DIR / 'training_metrics.json'}\")\n",
                "print(f\"  • Classification report: {RESULTS_DIR / 'classification_report.json'}\")\n",
                "print(f\"  • Confusion matrix: {RESULTS_DIR / 'confusion_matrix.png'}\")\n",
                "print(\"\\n✅ News classification completed successfully!\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
