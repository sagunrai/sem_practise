{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 2: News Classification\n",
                "\n",
                "This notebook implements:\n",
                "- Loading pretrained multilingual models (XLM-RoBERTa)\n",
                "- Fine-tuning for Nepali news classification\n",
                "- Training on 13+ news categories\n",
                "- Model evaluation and saving\n",
                "- Prediction on new articles"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %pip install numpy pandas matplotlib seaborn scikit-learn torch transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %pip install datasets evaluate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n",
                        "\n",
                        "‚úì Libraries imported successfully\n"
                    ]
                }
            ],
            "source": [
                "# Import required libraries\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Transformers and PyTorch\n",
                "import torch\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForSequenceClassification,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorWithPadding\n",
                ")\n",
                "from datasets import Dataset\n",
                "\n",
                "# Check GPU availability\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "print(\"\\n‚úì Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: FacebookAI/xlm-roberta-base\n",
                        "Max sequence length: 512\n",
                        "Batch size: 8\n",
                        "Learning rate: 2e-05\n",
                        "Number of epochs: 3\n"
                    ]
                }
            ],
            "source": [
                "# Paths\n",
                "BASE_DIR = Path(r'/workspaces/sem_practise')\n",
                "DATA_DIR = BASE_DIR / 'data' \n",
                "MODEL_DIR = BASE_DIR / 'models' / 'news_classifier'\n",
                "RESULTS_DIR = BASE_DIR / 'results'\n",
                "\n",
                "# Create directories\n",
                "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
                "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Model configuration\n",
                "MODEL_NAME = \"FacebookAI/xlm-roberta-base\"  # Pretrained multilingual model\n",
                "MAX_LENGTH = 512\n",
                "BATCH_SIZE = 8\n",
                "LEARNING_RATE = 2e-5\n",
                "NUM_EPOCHS = 3\n",
                "\n",
                "print(f\"Model: {MODEL_NAME}\")\n",
                "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
                "print(f\"Batch size: {BATCH_SIZE}\")\n",
                "print(f\"Learning rate: {LEARNING_RATE}\")\n",
                "print(f\"Number of epochs: {NUM_EPOCHS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Loaded 1431 training samples\n",
                        "‚úì Loaded 358 test samples\n",
                        "\n",
                        "Sample data:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>text</th>\n",
                            "      <th>category</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>‡•©‡•¶ ‡§™‡•Å‡§∏, ‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç ‡•§ ‡§¶‡•ã‡§∞‡•ç‡§¶‡•Ä ‡§ñ‡•ã‡§≤‡§æ ‡§ú‡§≤‡§µ‡§ø‡§¶‡•ç‡§Æ‡•Å‡§§ ‡§ï‡§Æ‡•ç‡§™‡§®...</td>\n",
                            "      <td>bank</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>‡§ï‡•á ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§•‡§æ‡§π‡§æ ‡§õ ? ‡§π‡§æ‡§•, ‡§ñ‡•Å‡§ü‡•ç‡§ü‡§æ‡§ï‡§æ ‡§®‡§ô‡§π‡§∞‡•Å‡§ï‡•ã ‡§∞‡§Ç‡§ó...</td>\n",
                            "      <td>health</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡•á‡§™‡•Ä ‡§∂‡§∞‡•ç‡§Æ‡§æ ‡§ì‡§≤‡•Ä ‡§•‡•ç‡§∞‡•Ä ‡§®‡•á‡§∏‡§®‡•ç‡§∏ ‡§ï‡§™‡§ï‡•ã ...</td>\n",
                            "      <td>sports</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>‡§∞‡§æ‡§ú‡§µ‡§ø‡§∞‡§æ‡§ú, ‡•®‡•™ ‡§ö‡•à‡§§ ‡•§ ‡§∏‡§™‡•ç‡§§‡§∞‡•Ä‡§ï‡•ã ‡§Æ‡§≤‡•á‡§ï‡§™‡•Å‡§∞ ‡§ó‡§æ‡§â‡§Å ‡§µ‡§ø‡§ï‡§æ‡§∏...</td>\n",
                            "      <td>politic</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>‡§∏‡•á‡§û‡•ç‡§ö‡•Å‡§∞‡•Ä‡§Æ‡§æ ‡§ó‡§§ ‡§Ö‡§∏‡•ã‡§ú ‡•ß ‡§ó‡§§‡•á‡§¶‡•á‡§ñ‡§ø ‡§™‡•ç‡§∞‡§Æ‡•Ç‡§ñ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§æ‡§∞‡•Ä ...</td>\n",
                            "      <td>bank</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                text category\n",
                            "0  ‡•©‡•¶ ‡§™‡•Å‡§∏, ‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç ‡•§ ‡§¶‡•ã‡§∞‡•ç‡§¶‡•Ä ‡§ñ‡•ã‡§≤‡§æ ‡§ú‡§≤‡§µ‡§ø‡§¶‡•ç‡§Æ‡•Å‡§§ ‡§ï‡§Æ‡•ç‡§™‡§®...     bank\n",
                            "1  ‡§ï‡•á ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§•‡§æ‡§π‡§æ ‡§õ ? ‡§π‡§æ‡§•, ‡§ñ‡•Å‡§ü‡•ç‡§ü‡§æ‡§ï‡§æ ‡§®‡§ô‡§π‡§∞‡•Å‡§ï‡•ã ‡§∞‡§Ç‡§ó...   health\n",
                            "2  ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡•á‡§™‡•Ä ‡§∂‡§∞‡•ç‡§Æ‡§æ ‡§ì‡§≤‡•Ä ‡§•‡•ç‡§∞‡•Ä ‡§®‡•á‡§∏‡§®‡•ç‡§∏ ‡§ï‡§™‡§ï‡•ã ...   sports\n",
                            "3  ‡§∞‡§æ‡§ú‡§µ‡§ø‡§∞‡§æ‡§ú, ‡•®‡•™ ‡§ö‡•à‡§§ ‡•§ ‡§∏‡§™‡•ç‡§§‡§∞‡•Ä‡§ï‡•ã ‡§Æ‡§≤‡•á‡§ï‡§™‡•Å‡§∞ ‡§ó‡§æ‡§â‡§Å ‡§µ‡§ø‡§ï‡§æ‡§∏...  politic\n",
                            "4  ‡§∏‡•á‡§û‡•ç‡§ö‡•Å‡§∞‡•Ä‡§Æ‡§æ ‡§ó‡§§ ‡§Ö‡§∏‡•ã‡§ú ‡•ß ‡§ó‡§§‡•á‡§¶‡•á‡§ñ‡§ø ‡§™‡•ç‡§∞‡§Æ‡•Ç‡§ñ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§æ‡§∞‡•Ä ...     bank"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Load preprocessed data\n",
                "with open(DATA_DIR / 'train_data.json', 'r', encoding='utf-8') as f:\n",
                "    train_data = json.load(f)\n",
                "\n",
                "with open(DATA_DIR / 'test_data.json', 'r', encoding='utf-8') as f:\n",
                "    test_data = json.load(f)\n",
                "\n",
                "print(f\"‚úì Loaded {len(train_data)} training samples\")\n",
                "print(f\"‚úì Loaded {len(test_data)} test samples\")\n",
                "\n",
                "# Convert to DataFrames for easier handling\n",
                "train_df = pd.DataFrame(train_data)\n",
                "test_df = pd.DataFrame(test_data)\n",
                "\n",
                "print(\"\\nSample data:\")\n",
                "train_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of categories: 13\n",
                        "\n",
                        "Category mappings:\n",
                        "  0: Agriculture\n",
                        "  1: automobiles\n",
                        "  2: bank\n",
                        "  3: business\n",
                        "  4: economy\n",
                        "  5: education\n",
                        "  6: entertainment\n",
                        "  7: health\n",
                        "  8: politic\n",
                        "  9: sports\n",
                        "  10: technology\n",
                        "  11: tourism\n",
                        "  12: world\n",
                        "\n",
                        "‚úì Label mappings saved to /workspaces/sem_practise/models/news_classifier/label_mappings.json\n"
                    ]
                }
            ],
            "source": [
                "# Create label mappings\n",
                "categories = sorted(train_df['category'].unique())\n",
                "label2id = {label: idx for idx, label in enumerate(categories)}\n",
                "id2label = {idx: label for label, idx in label2id.items()}\n",
                "\n",
                "print(f\"Number of categories: {len(categories)}\")\n",
                "print(\"\\nCategory mappings:\")\n",
                "for label, idx in label2id.items():\n",
                "    print(f\"  {idx}: {label}\")\n",
                "\n",
                "# Add numeric labels\n",
                "train_df['label'] = train_df['category'].map(label2id)\n",
                "test_df['label'] = test_df['category'].map(label2id)\n",
                "\n",
                "# Save label mappings\n",
                "with open(MODEL_DIR / 'label_mappings.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump({'label2id': label2id, 'id2label': id2label}, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"\\n‚úì Label mappings saved to {MODEL_DIR / 'label_mappings.json'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Pretrained Model and Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                }
            ],
            "source": [
                "# Load the pretrained model and tokenizer\n",
                "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
                "\n",
                "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Tokenize Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokenizing training data...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1431/1431 [00:02<00:00, 653.69 examples/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokenizing test data...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 358/358 [00:00<00:00, 718.07 examples/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚úì Tokenization complete\n",
                        "Train dataset: Dataset({\n",
                        "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
                        "    num_rows: 1431\n",
                        "})\n",
                        "Test dataset: Dataset({\n",
                        "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
                        "    num_rows: 358\n",
                        "})\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "def tokenize_function(examples):\n",
                "    \"\"\"Tokenize text data\"\"\"\n",
                "    return tokenizer(\n",
                "        examples['text'],\n",
                "        padding='max_length',\n",
                "        truncation=True,\n",
                "        max_length=MAX_LENGTH\n",
                "    )\n",
                "\n",
                "# Convert to HuggingFace Dataset format\n",
                "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
                "test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
                "\n",
                "print(\"Tokenizing training data...\")\n",
                "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
                "\n",
                "print(\"Tokenizing test data...\")\n",
                "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
                "\n",
                "print(\"\\n‚úì Tokenization complete\")\n",
                "print(f\"Train dataset: {train_dataset}\")\n",
                "print(f\"Test dataset: {test_dataset}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training configuration:\n",
                        "  Output directory: /workspaces/sem_practise/models/news_classifier/checkpoints\n",
                        "  Epochs: 3\n",
                        "  Batch size: 8\n",
                        "  Learning rate: 2e-05\n",
                        "  Mixed precision (FP16): False\n"
                    ]
                }
            ],
            "source": [
                "# Define training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=str(MODEL_DIR / 'checkpoints'),\n",
                "    num_train_epochs=NUM_EPOCHS,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    per_device_eval_batch_size=BATCH_SIZE,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    weight_decay=0.01,\n",
                "    eval_strategy='epoch',\n",
                "    save_strategy='epoch',\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model='accuracy',\n",
                "    logging_dir=str(MODEL_DIR / 'logs'),\n",
                "    logging_steps=10,\n",
                "    warmup_steps=100,\n",
                "    save_total_limit=2,\n",
                "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
                ")\n",
                "\n",
                "print(\"Training configuration:\")\n",
                "print(f\"  Output directory: {training_args.output_dir}\")\n",
                "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
                "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
                "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
                "print(f\"  Mixed precision (FP16): {training_args.fp16}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %pip install transformers[torch]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Metrics and data collator configured\n"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "# Define metrics\n",
                "def compute_metrics(eval_pred):\n",
                "    \"\"\"Compute accuracy and other metrics\"\"\"\n",
                "    predictions, labels = eval_pred\n",
                "    predictions = np.argmax(predictions, axis=1)\n",
                "    accuracy = accuracy_score(labels, predictions)\n",
                "    return {'accuracy': accuracy}\n",
                "\n",
                "# Data collator\n",
                "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
                "\n",
                "print(\"‚úì Metrics and data collator configured\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Fine-tune Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Trainer initialized\n",
                        "\n",
                        "Starting fine-tuning...\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Initialize Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=test_dataset,\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics,\n",
                ")\n",
                "\n",
                "print(\"‚úì Trainer initialized\")\n",
                "print(\"\\nStarting fine-tuning...\\n\")\n",
                "\n",
                "# Train the model\n",
                "train_result = trainer.train()\n",
                "\n",
                "print(\"\\n‚úì Training complete!\")\n",
                "print(f\"Training loss: {train_result.training_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Evaluate Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set\n",
                "print(\"Evaluating model on test set...\")\n",
                "eval_results = trainer.evaluate()\n",
                "\n",
                "print(\"\\nEvaluation Results:\")\n",
                "for key, value in eval_results.items():\n",
                "    print(f\"  {key}: {value:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get predictions\n",
                "predictions = trainer.predict(test_dataset)\n",
                "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
                "true_labels = test_df['label'].values\n",
                "\n",
                "# Classification report\n",
                "print(\"\\nDetailed Classification Report:\")\n",
                "print(\"=\"*80)\n",
                "report = classification_report(\n",
                "    true_labels,\n",
                "    pred_labels,\n",
                "    target_names=categories,\n",
                "    digits=4\n",
                ")\n",
                "print(report)\n",
                "\n",
                "# Save classification report\n",
                "report_dict = classification_report(\n",
                "    true_labels,\n",
                "    pred_labels,\n",
                "    target_names=categories,\n",
                "    output_dict=True\n",
                ")\n",
                "\n",
                "with open(RESULTS_DIR / 'classification_report.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(report_dict, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"\\n‚úì Report saved to {RESULTS_DIR / 'classification_report.json'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "cm = confusion_matrix(true_labels, pred_labels)\n",
                "\n",
                "plt.figure(figsize=(14, 12))\n",
                "sns.heatmap(\n",
                "    cm,\n",
                "    annot=True,\n",
                "    fmt='d',\n",
                "    cmap='Blues',\n",
                "    xticklabels=categories,\n",
                "    yticklabels=categories,\n",
                "    cbar_kws={'label': 'Count'}\n",
                ")\n",
                "plt.title('Confusion Matrix - News Classification', fontsize=16, fontweight='bold', pad=20)\n",
                "plt.xlabel('Predicted Category', fontsize=12)\n",
                "plt.ylabel('True Category', fontsize=12)\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.yticks(rotation=0)\n",
                "plt.tight_layout()\n",
                "plt.savefig(RESULTS_DIR / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"‚úì Confusion matrix saved to {RESULTS_DIR / 'confusion_matrix.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Fine-tuned Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the fine-tuned model\n",
                "print(\"Saving fine-tuned model...\")\n",
                "trainer.save_model(str(MODEL_DIR / 'final_model'))\n",
                "tokenizer.save_pretrained(str(MODEL_DIR / 'final_model'))\n",
                "\n",
                "print(f\"‚úì Model saved to {MODEL_DIR / 'final_model'}\")\n",
                "\n",
                "# Save training metrics\n",
                "metrics = {\n",
                "    'model_name': MODEL_NAME,\n",
                "    'num_categories': len(categories),\n",
                "    'train_samples': len(train_df),\n",
                "    'test_samples': len(test_df),\n",
                "    'accuracy': float(eval_results['eval_accuracy']),\n",
                "    'training_loss': float(train_result.training_loss),\n",
                "    'epochs': NUM_EPOCHS,\n",
                "    'batch_size': BATCH_SIZE,\n",
                "    'learning_rate': LEARNING_RATE\n",
                "}\n",
                "\n",
                "with open(MODEL_DIR / 'training_metrics.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"‚úì Metrics saved to {MODEL_DIR / 'training_metrics.json'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Test Predictions on Sample Articles"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_category(text, model, tokenizer, id2label, device):\n",
                "    \"\"\"\n",
                "    Predict category for a given text\n",
                "    \"\"\"\n",
                "    # Tokenize\n",
                "    inputs = tokenizer(\n",
                "        text,\n",
                "        return_tensors='pt',\n",
                "        truncation=True,\n",
                "        max_length=MAX_LENGTH,\n",
                "        padding=True\n",
                "    ).to(device)\n",
                "    \n",
                "    # Predict\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
                "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
                "        confidence = predictions[0][predicted_class].item()\n",
                "    \n",
                "    return id2label[str(predicted_class)], confidence\n",
                "\n",
                "# Test on random samples\n",
                "print(\"Testing predictions on random samples:\\n\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "sample_indices = np.random.choice(len(test_df), 5, replace=False)\n",
                "\n",
                "for idx in sample_indices:\n",
                "    text = test_df.iloc[idx]['text']\n",
                "    true_category = test_df.iloc[idx]['category']\n",
                "    \n",
                "    predicted_category, confidence = predict_category(text, model, tokenizer, id2label, device)\n",
                "    \n",
                "    print(f\"\\nText (first 150 chars): {text[:150]}...\")\n",
                "    print(f\"True Category: {true_category}\")\n",
                "    print(f\"Predicted Category: {predicted_category}\")\n",
                "    print(f\"Confidence: {confidence:.4f}\")\n",
                "    print(f\"Correct: {'‚úì' if true_category == predicted_category else '‚úó'}\")\n",
                "    print(\"-\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"NEWS CLASSIFICATION SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "print(f\"\\nü§ñ Model: {MODEL_NAME} (Pretrained)\")\n",
                "print(f\"üìä Categories: {len(categories)}\")\n",
                "print(f\"üìù Training samples: {len(train_df)}\")\n",
                "print(f\"üß™ Test samples: {len(test_df)}\")\n",
                "print(f\"\\nüìà Performance:\")\n",
                "print(f\"  ‚Ä¢ Accuracy: {metrics['accuracy']:.4f}\")\n",
                "print(f\"  ‚Ä¢ Training loss: {metrics['training_loss']:.4f}\")\n",
                "print(f\"\\nüíæ Saved Files:\")\n",
                "print(f\"  ‚Ä¢ Model: {MODEL_DIR / 'final_model'}\")\n",
                "print(f\"  ‚Ä¢ Label mappings: {MODEL_DIR / 'label_mappings.json'}\")\n",
                "print(f\"  ‚Ä¢ Training metrics: {MODEL_DIR / 'training_metrics.json'}\")\n",
                "print(f\"  ‚Ä¢ Classification report: {RESULTS_DIR / 'classification_report.json'}\")\n",
                "print(f\"  ‚Ä¢ Confusion matrix: {RESULTS_DIR / 'confusion_matrix.png'}\")\n",
                "print(\"\\n‚úÖ News classification completed successfully!\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
