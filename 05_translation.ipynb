{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 5: Translation (Nepali ‚Üî English)\n",
                "\n",
                "This notebook implements:\n",
                "- Using pretrained Helsinki-NLP models for translation\n",
                "- Nepali to English translation\n",
                "- English to Nepali translation\n",
                "- Batch translation support\n",
                "- Translation quality evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import torch\n",
                "from transformers import MarianMTModel, MarianTokenizer, pipeline\n",
                "from tqdm import tqdm\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "print(\"‚úì Libraries imported\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths\n",
                "BASE_DIR = Path(r'c:\\Users\\sagun\\Desktop\\news_project')\n",
                "DATA_DIR = BASE_DIR / 'data' / 'processed'\n",
                "MODEL_DIR = BASE_DIR / 'models' / 'translator'\n",
                "RESULTS_DIR = BASE_DIR / 'results' / 'translations'\n",
                "\n",
                "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
                "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Pretrained translation models\n",
                "# Note: Direct Nepali models may not be available, so we'll use Indic language models\n",
                "NE_TO_EN_MODEL = \"Helsinki-NLP/opus-mt-mul-en\"  # Multilingual to English\n",
                "EN_TO_NE_MODEL = \"Helsinki-NLP/opus-mt-en-mul\"  # English to Multilingual\n",
                "\n",
                "print(f\"Nepali‚ÜíEnglish Model: {NE_TO_EN_MODEL}\")\n",
                "print(f\"English‚ÜíNepali Model: {EN_TO_NE_MODEL}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Pretrained Translation Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Nepali to English model\n",
                "print(f\"Loading Nepali‚ÜíEnglish model: {NE_TO_EN_MODEL}...\")\n",
                "ne_to_en_tokenizer = MarianTokenizer.from_pretrained(NE_TO_EN_MODEL)\n",
                "ne_to_en_model = MarianMTModel.from_pretrained(NE_TO_EN_MODEL).to(device)\n",
                "print(\"‚úì Nepali‚ÜíEnglish model loaded\")\n",
                "\n",
                "# Load English to Nepali model\n",
                "print(f\"\\nLoading English‚ÜíNepali model: {EN_TO_NE_MODEL}...\")\n",
                "en_to_ne_tokenizer = MarianTokenizer.from_pretrained(EN_TO_NE_MODEL)\n",
                "en_to_ne_model = MarianMTModel.from_pretrained(EN_TO_NE_MODEL).to(device)\n",
                "print(\"‚úì English‚ÜíNepali model loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Translation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def translate_ne_to_en(text, max_length=512):\n",
                "    \"\"\"\n",
                "    Translate Nepali text to English using pretrained model\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Tokenize\n",
                "        inputs = ne_to_en_tokenizer(\n",
                "            text,\n",
                "            return_tensors=\"pt\",\n",
                "            padding=True,\n",
                "            truncation=True,\n",
                "            max_length=max_length\n",
                "        ).to(device)\n",
                "        \n",
                "        # Translate\n",
                "        ne_to_en_model.eval()\n",
                "        with torch.no_grad():\n",
                "            translated = ne_to_en_model.generate(**inputs, max_length=max_length)\n",
                "        \n",
                "        # Decode\n",
                "        translation = ne_to_en_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
                "        return translation\n",
                "    except Exception as e:\n",
                "        print(f\"Translation error: {e}\")\n",
                "        return \"\"\n",
                "\n",
                "def translate_en_to_ne(text, max_length=512):\n",
                "    \"\"\"\n",
                "    Translate English text to Nepali using pretrained model\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Tokenize\n",
                "        inputs = en_to_ne_tokenizer(\n",
                "            text,\n",
                "            return_tensors=\"pt\",\n",
                "            padding=True,\n",
                "            truncation=True,\n",
                "            max_length=max_length\n",
                "        ).to(device)\n",
                "        \n",
                "        # Translate\n",
                "        en_to_ne_model.eval()\n",
                "        with torch.no_grad():\n",
                "            translated = en_to_ne_model.generate(**inputs, max_length=max_length)\n",
                "        \n",
                "        # Decode\n",
                "        translation = en_to_ne_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
                "        return translation\n",
                "    except Exception as e:\n",
                "        print(f\"Translation error: {e}\")\n",
                "        return \"\"\n",
                "\n",
                "# Test translations\n",
                "print(\"Testing translation functions:\\n\")\n",
                "\n",
                "nepali_text = \"‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Ç‡§Æ‡§æ ‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§õ‡•§\"\n",
                "english_text = \"The weather is good in Kathmandu today.\"\n",
                "\n",
                "print(f\"Nepali: {nepali_text}\")\n",
                "print(f\"‚Üí English: {translate_ne_to_en(nepali_text)}\")\n",
                "print(f\"\\nEnglish: {english_text}\")\n",
                "print(f\"‚Üí Nepali: {translate_en_to_ne(english_text)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test data\n",
                "with open(DATA_DIR / 'test_data.json', 'r', encoding='utf-8') as f:\n",
                "    test_data = json.load(f)\n",
                "\n",
                "df = pd.DataFrame(test_data)\n",
                "print(f\"‚úì Loaded {len(df)} articles\")\n",
                "\n",
                "# Use a sample for demonstration\n",
                "sample_size = min(30, len(df))\n",
                "sample_df = df.sample(n=sample_size, random_state=42).copy()\n",
                "print(f\"‚úì Using {sample_size} articles for translation\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Translate Articles"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Translate Nepali articles to English\n",
                "print(\"Translating Nepali articles to English...\\n\")\n",
                "\n",
                "translations = []\n",
                "\n",
                "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Translating\"):\n",
                "    nepali_text = row['text']\n",
                "    \n",
                "    # Translate first 200 words for efficiency\n",
                "    words = nepali_text.split()[:200]\n",
                "    text_to_translate = ' '.join(words)\n",
                "    \n",
                "    english_translation = translate_ne_to_en(text_to_translate)\n",
                "    \n",
                "    translations.append({\n",
                "        'original_nepali': nepali_text,\n",
                "        'english_translation': english_translation,\n",
                "        'category': row['category'],\n",
                "        'original_length': len(nepali_text),\n",
                "        'translation_length': len(english_translation)\n",
                "    })\n",
                "\n",
                "translations_df = pd.DataFrame(translations)\n",
                "print(\"\\n‚úì Translation complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Analyze Translations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate statistics\n",
                "print(\"Translation Statistics:\\n\")\n",
                "print(f\"Total translations: {len(translations_df)}\")\n",
                "print(f\"Average original length: {translations_df['original_length'].mean():.0f} chars\")\n",
                "print(f\"Average translation length: {translations_df['translation_length'].mean():.0f} chars\")\n",
                "print(f\"Average length ratio: {(translations_df['translation_length'] / translations_df['original_length']).mean():.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize translation statistics\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Length comparison\n",
                "axes[0].scatter(translations_df['original_length'], translations_df['translation_length'], \n",
                "                alpha=0.6, s=50, color='steelblue')\n",
                "axes[0].plot([0, translations_df['original_length'].max()], \n",
                "             [0, translations_df['original_length'].max()], \n",
                "             'r--', label='1:1 ratio')\n",
                "axes[0].set_title('Original vs Translation Length', fontweight='bold', fontsize=12)\n",
                "axes[0].set_xlabel('Original Length (Nepali)')\n",
                "axes[0].set_ylabel('Translation Length (English)')\n",
                "axes[0].legend()\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "# Length ratio distribution\n",
                "length_ratio = translations_df['translation_length'] / translations_df['original_length']\n",
                "axes[1].hist(length_ratio, bins=20, color='lightcoral', edgecolor='black')\n",
                "axes[1].set_title('Translation Length Ratio Distribution', fontweight='bold', fontsize=12)\n",
                "axes[1].set_xlabel('Translation/Original Length Ratio')\n",
                "axes[1].set_ylabel('Frequency')\n",
                "axes[1].axvline(length_ratio.mean(), color='red', linestyle='--', label=f'Mean: {length_ratio.mean():.2f}')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(RESULTS_DIR / 'translation_statistics.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"‚úì Visualization saved to {RESULTS_DIR / 'translation_statistics.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save translations\n",
                "translations_output = translations_df.to_dict('records')\n",
                "\n",
                "with open(RESULTS_DIR / 'nepali_to_english_translations.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(translations_output, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"‚úì Translations saved to {RESULTS_DIR / 'nepali_to_english_translations.json'}\")\n",
                "\n",
                "# Save statistics\n",
                "stats = {\n",
                "    'total_translations': len(translations_df),\n",
                "    'avg_original_length': float(translations_df['original_length'].mean()),\n",
                "    'avg_translation_length': float(translations_df['translation_length'].mean()),\n",
                "    'avg_length_ratio': float(length_ratio.mean()),\n",
                "    'models_used': {\n",
                "        'nepali_to_english': NE_TO_EN_MODEL,\n",
                "        'english_to_nepali': EN_TO_NE_MODEL\n",
                "    }\n",
                "}\n",
                "\n",
                "with open(RESULTS_DIR / 'translation_statistics.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(stats, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"‚úì Statistics saved to {RESULTS_DIR / 'translation_statistics.json'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Example Translations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display example translations\n",
                "print(\"=\"*80)\n",
                "print(\"EXAMPLE TRANSLATIONS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "for i in range(min(5, len(translations_df))):\n",
                "    row = translations_df.iloc[i]\n",
                "    \n",
                "    print(f\"\\nüì∞ Article {i+1} - Category: {row['category']}\")\n",
                "    print(\"-\" * 80)\n",
                "    print(f\"\\nOriginal (Nepali):\\n{row['original_nepali'][:300]}...\\n\")\n",
                "    print(f\"Translation (English):\\n{row['english_translation'][:300]}...\")\n",
                "    print(\"\\n\" + \"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Cross-Language Summarization Demo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate cross-language summarization\n",
                "# (Translate Nepali ‚Üí English ‚Üí Summarize ‚Üí Translate back to Nepali)\n",
                "\n",
                "print(\"Cross-Language Summarization Demo:\\n\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "sample_article = sample_df.iloc[0]['text']\n",
                "\n",
                "print(\"Step 1: Original Nepali Article\")\n",
                "print(f\"{sample_article[:200]}...\\n\")\n",
                "\n",
                "print(\"Step 2: Translate to English\")\n",
                "english_version = translate_ne_to_en(sample_article)\n",
                "print(f\"{english_version[:200]}...\\n\")\n",
                "\n",
                "print(\"Step 3: Translate back to Nepali\")\n",
                "back_to_nepali = translate_en_to_ne(english_version)\n",
                "print(f\"{back_to_nepali[:200]}...\\n\")\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"This demonstrates the capability for cross-language processing!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"TRANSLATION SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "print(f\"\\nü§ñ Pretrained Models:\")\n",
                "print(f\"  ‚Ä¢ Nepali‚ÜíEnglish: {NE_TO_EN_MODEL}\")\n",
                "print(f\"  ‚Ä¢ English‚ÜíNepali: {EN_TO_NE_MODEL}\")\n",
                "print(f\"\\nüìä Translation Statistics:\")\n",
                "print(f\"  ‚Ä¢ Total translations: {stats['total_translations']}\")\n",
                "print(f\"  ‚Ä¢ Avg original length: {stats['avg_original_length']:.0f} chars\")\n",
                "print(f\"  ‚Ä¢ Avg translation length: {stats['avg_translation_length']:.0f} chars\")\n",
                "print(f\"  ‚Ä¢ Avg length ratio: {stats['avg_length_ratio']:.2f}\")\n",
                "print(f\"\\nüíæ Saved Files:\")\n",
                "print(f\"  ‚Ä¢ Translations: {RESULTS_DIR / 'nepali_to_english_translations.json'}\")\n",
                "print(f\"  ‚Ä¢ Statistics: {RESULTS_DIR / 'translation_statistics.json'}\")\n",
                "print(f\"  ‚Ä¢ Visualization: {RESULTS_DIR / 'translation_statistics.png'}\")\n",
                "print(\"\\n‚úÖ Translation completed successfully!\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}